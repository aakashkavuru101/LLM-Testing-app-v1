#!/usr/bin/env python3
"""
Quick Start Guide for LLM Testing Automation
Run this script to get started with the system
"""

import os
import sys

def print_banner():
    print("üöÄ LLM Testing Automation System")
    print("=" * 50)
    print("Connect to LMArena/FastChat locally for free LLM testing!")
    print()

def check_files():
    print("üìÅ Checking required files...")
    files = {
        'llm_testing_automation.py': 'Main automation script',
        'server_manager.py': 'FastChat server management',
        'LLM_testing.xlsx': 'Sample test prompts',
        'README_LLM_TESTING.md': 'Complete documentation'
    }
    
    all_present = True
    for file, description in files.items():
        if os.path.exists(file):
            print(f"‚úÖ {file} - {description}")
        else:
            print(f"‚ùå {file} - {description} (MISSING)")
            all_present = False
    
    return all_present

def show_quick_start():
    print("\nüöÄ Quick Start Options:")
    print()
    print("1Ô∏è‚É£  RUN DEMO (No model download needed):")
    print("   python demo.py")
    print()
    print("2Ô∏è‚É£  CREATE TEST EXCEL FILE:")
    print("   python create_sample_excel.py")
    print()
    print("3Ô∏è‚É£  RUN FULL SYSTEM (Downloads model if needed):")
    print("   python llm_testing_automation.py")
    print()
    print("4Ô∏è‚É£  RUN TESTS TO VALIDATE SETUP:")
    print("   python test_system.py")
    print()

def show_manual_setup():
    print("üîß Manual Setup (if you prefer):")
    print()
    print("Terminal 1: python -m fastchat.serve.controller")
    print("Terminal 2: python -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5") 
    print("Terminal 3: python -m fastchat.serve.openai_api_server --host localhost --port 8000")
    print("Terminal 4: python llm_testing_automation.py")
    print()

def show_excel_format():
    print("üìä Excel File Format:")
    print("Your Excel file should have these columns:")
    columns = [
        'Company', 'Model/LLM name', 'Category', 'framework for prompting',
        'Prompt approach/theme', 'user system prompt', 'actual user prompt',
        'Expected behavioural response', 'override responses'
    ]
    for i, col in enumerate(columns, 1):
        print(f"  {i:2d}. {col}")
    print()

def show_troubleshooting():
    print("üõ†Ô∏è  Common Issues:")
    print()
    print("‚ùì Port conflicts (Error 10048):")
    print("   ‚Üí The system automatically handles this")
    print("   ‚Üí Or run: python -c \"from server_manager import ServerManager; ServerManager().cleanup_existing_servers()\"")
    print()
    print("‚ùì Model download slow/fails:")
    print("   ‚Üí Ensure stable internet connection")
    print("   ‚Üí Check available disk space (7B models need ~13GB)")
    print("   ‚Üí Try smaller model: fastchat-t5-3b-v1.0")
    print()
    print("‚ùì Out of memory:")
    print("   ‚Üí Use smaller models")
    print("   ‚Üí Close other applications")
    print("   ‚Üí Consider cloud/GPU instances")
    print()

def main():
    print_banner()
    
    if not check_files():
        print("\n‚ùå Some required files are missing. Please check your installation.")
        return
    
    show_quick_start()
    show_excel_format()
    show_manual_setup()
    show_troubleshooting()
    
    print("üìñ For detailed documentation, see: README_LLM_TESTING.md")
    print()
    print("üéØ Recommended first step: python demo.py")
    print("   (Shows the system working without downloading models)")

if __name__ == "__main__":
    main()